{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tempfile import mkdtemp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals.joblib import Memory\n",
    "from clean import ReviewCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating memory for pipeline to avoid redundant processing.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    cachedir = mkdtemp()\n",
    "    memory = Memory(cachedir=cachedir, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaner for cleaning reviews.\n",
    "cleaner = ReviewCleaner()\n",
    "# vectoriser for word embedding.\n",
    "vectoriser = TfidfVectorizer()\n",
    "# decomposer for data compression.\n",
    "decomposer = TruncatedSVD(n_components=100)\n",
    "# selector for dimensionality reduction.\n",
    "selector = SelectKBest(mutual_info_classif, k=10)\n",
    "# predictor for modelling target.\n",
    "predictor = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data.\n",
    "train = pd.read_csv(\"../data/raw/labeledTrainData.tsv\", sep='\\t')\n",
    "X_train = train[\"review\"]\n",
    "y_train = train[\"sentiment\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading testing data.\n",
    "test = pd.read_csv(\"../data/raw/testData.tsv\", sep='\\t')\n",
    "X_test = test[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pipeline.\n",
    "p = Pipeline([\n",
    "    ('cleaner', cleaner),\n",
    "    ('vectoriser', vectoriser),\n",
    "    ('decomposer', decomposer),\n",
    "    ('selector', selector),\n",
    "    ('predictor', predictor)\n",
    "], memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameters for grid search.\n",
    "params = {\n",
    "    \"vectoriser__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"predictor__C\": 10.**np.arange(-2, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining grid search with cross validation.\n",
    "learner = GridSearchCV(p, params, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from C:\\Users\\Jan\\AppData\\Local\\Temp\\tmpqi1fny7a\\joblib\\sklearn\\pipeline\\_fit_transform_one\\94f528d72cd78612791234c5d06eff36\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), \n",
      "0     stuff moment mj start listen music watch odd d...\n",
      "1     classic war world timothy hines entertaining f...\n",
      "2     film start manager nicholas bell give welcome ...\n",
      "3     must assume praise film great filmed opera eve...\n",
      "4     superbly trashy wondrously unpretentious explo...\n",
      "5     dont know people think bad movie pretty good p...\n",
      "6     movie good way short cheesy special effect act...\n",
      "7     watch video friend house glad waste money buy ...\n",
      "8     friend mine bought film grossly overpriced des...\n",
      "9     movie full reference like mad max ii wild many...\n",
      "10    happen army wetback towelheads godless eastern...\n",
      "11    although generally like remake believe remake ...\n",
      "12    mr harvey light candle..., \n",
      "0      True\n",
      "1      True\n",
      "2     False\n",
      "3     False\n",
      "4      True\n",
      "5      True\n",
      "6     False\n",
      "7     False\n",
      "8     False\n",
      "9      True\n",
      "10    False\n",
      "11     True\n",
      "12     True\n",
      "13    False\n",
      "14    False\n",
      "15    False\n",
      "16    False\n",
      "17    False\n",
      "18     True\n",
      "19     True\n",
      "20     True\n",
      "21     True\n",
      "22     True\n",
      "23    False\n",
      "24    False\n",
      "25     True\n",
      "26    False\n",
      "27    False\n",
      "28    False\n",
      "29    False\n",
      "      ...  \n",
      "70     True\n",
      "71    False\n",
      "72    False\n",
      "73    False\n",
      "74    False\n",
      "75     True\n",
      "76    False\n",
      "77     True\n",
      "78     True\n",
      "79     True\n",
      "80     True\n",
      "81     True\n",
      "82    False\n",
      "83    False\n",
      "84     True\n",
      "85    False\n",
      "86     True\n",
      "87    False\n",
      "88     True\n",
      "89    False\n",
      "90    False\n",
      "91    False\n",
      "92     True\n",
      "93    False\n",
      "94     True\n",
      "95    False\n",
      "96     True\n",
      "97    False\n",
      "98    False\n",
      "99    False\n",
      "Name: sentiment, Length: 100, dtype: bool, \n",
      "None)\n",
      "________________________________________________fit_transform_one - 0.1s, 0.0min\n",
      "[Memory]0.6s, 0.0min    : Loading _fit_transform_one from C:\\Users\\Jan\\AppData\\Local\\Temp\\tmpqi1fny7a\\joblib\\sklearn\\pipeline\\_fit_transform_one\\7ed9b5be689e1545cf2fae18f2aae4c9\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.6s, 0.0min    : Loading _fit_transform_one from C:\\Users\\Jan\\AppData\\Local\\Temp\\tmpqi1fny7a\\joblib\\sklearn\\pipeline\\_fit_transform_one\\ed0d9527e954614882275d764bf93e69\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# fitting learner to training data (only a sample of 100 to accelerate learning process).\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    learner.fit(X_train.head(100), y_train.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_predictor__C</th>\n",
       "      <th>param_vectoriser__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077291</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>2.083854</td>\n",
       "      <td>0.116115</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'predictor__C': 0.01, 'vectoriser__ngram_rang...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.171042</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>2.092723</td>\n",
       "      <td>0.165722</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'predictor__C': 0.01, 'vectoriser__ngram_rang...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079287</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>2.096653</td>\n",
       "      <td>0.104976</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'predictor__C': 0.1, 'vectoriser__ngram_range...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>2.080418</td>\n",
       "      <td>0.109221</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'predictor__C': 0.1, 'vectoriser__ngram_range...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134640</td>\n",
       "      <td>0.042885</td>\n",
       "      <td>2.344731</td>\n",
       "      <td>0.042885</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'predictor__C': 1.0, 'vectoriser__ngram_range...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.217915</td>\n",
       "      <td>0.026430</td>\n",
       "      <td>2.374651</td>\n",
       "      <td>0.072805</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'predictor__C': 1.0, 'vectoriser__ngram_range...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.550020</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.126161</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>2.365675</td>\n",
       "      <td>0.300197</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'predictor__C': 10.0, 'vectoriser__ngram_rang...</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.021804</td>\n",
       "      <td>1</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.758303</td>\n",
       "      <td>0.084834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.186002</td>\n",
       "      <td>0.011469</td>\n",
       "      <td>2.383129</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'predictor__C': 10.0, 'vectoriser__ngram_rang...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.031606</td>\n",
       "      <td>2</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.758303</td>\n",
       "      <td>0.084834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.083776</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>1.541022</td>\n",
       "      <td>0.100731</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'predictor__C': 100.0, 'vectoriser__ngram_ran...</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.031606</td>\n",
       "      <td>2</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.016807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.185005</td>\n",
       "      <td>0.010473</td>\n",
       "      <td>1.432814</td>\n",
       "      <td>0.111205</td>\n",
       "      <td>100</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{'predictor__C': 100.0, 'vectoriser__ngram_ran...</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.081016</td>\n",
       "      <td>4</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.808123</td>\n",
       "      <td>0.093838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.077291      0.004488         2.083854        0.116115   \n",
       "1       0.171042      0.005485         2.092723        0.165722   \n",
       "2       0.079287      0.003491         2.096653        0.104976   \n",
       "3       0.174035      0.003491         2.080418        0.109221   \n",
       "4       0.134640      0.042885         2.344731        0.042885   \n",
       "5       0.217915      0.026430         2.374651        0.072805   \n",
       "6       0.126161      0.008478         2.365675        0.300197   \n",
       "7       0.186002      0.011469         2.383129        0.018450   \n",
       "8       0.083776      0.002993         1.541022        0.100731   \n",
       "9       0.185005      0.010473         1.432814        0.111205   \n",
       "\n",
       "  param_predictor__C param_vectoriser__ngram_range  \\\n",
       "0               0.01                        (1, 1)   \n",
       "1               0.01                        (1, 2)   \n",
       "2                0.1                        (1, 1)   \n",
       "3                0.1                        (1, 2)   \n",
       "4                  1                        (1, 1)   \n",
       "5                  1                        (1, 2)   \n",
       "6                 10                        (1, 1)   \n",
       "7                 10                        (1, 2)   \n",
       "8                100                        (1, 1)   \n",
       "9                100                        (1, 2)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'predictor__C': 0.01, 'vectoriser__ngram_rang...           0.549020   \n",
       "1  {'predictor__C': 0.01, 'vectoriser__ngram_rang...           0.549020   \n",
       "2  {'predictor__C': 0.1, 'vectoriser__ngram_range...           0.549020   \n",
       "3  {'predictor__C': 0.1, 'vectoriser__ngram_range...           0.549020   \n",
       "4  {'predictor__C': 1.0, 'vectoriser__ngram_range...           0.549020   \n",
       "5  {'predictor__C': 1.0, 'vectoriser__ngram_range...           0.549020   \n",
       "6  {'predictor__C': 10.0, 'vectoriser__ngram_rang...           0.568627   \n",
       "7  {'predictor__C': 10.0, 'vectoriser__ngram_rang...           0.549020   \n",
       "8  {'predictor__C': 100.0, 'vectoriser__ngram_ran...           0.549020   \n",
       "9  {'predictor__C': 100.0, 'vectoriser__ngram_ran...           0.470588   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.551020             0.55        0.001000                4   \n",
       "1           0.551020             0.55        0.001000                4   \n",
       "2           0.551020             0.55        0.001000                4   \n",
       "3           0.551020             0.55        0.001000                4   \n",
       "4           0.551020             0.55        0.001000                4   \n",
       "5           0.551020             0.55        0.001000                4   \n",
       "6           0.612245             0.59        0.021804                1   \n",
       "7           0.612245             0.58        0.031606                2   \n",
       "8           0.612245             0.58        0.031606                2   \n",
       "9           0.632653             0.55        0.081016                4   \n",
       "\n",
       "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "0            0.551020            0.549020          0.550020         0.001000  \n",
       "1            0.551020            0.549020          0.550020         0.001000  \n",
       "2            0.551020            0.549020          0.550020         0.001000  \n",
       "3            0.551020            0.549020          0.550020         0.001000  \n",
       "4            0.551020            0.549020          0.550020         0.001000  \n",
       "5            0.551020            0.549020          0.550020         0.001000  \n",
       "6            0.673469            0.843137          0.758303         0.084834  \n",
       "7            0.673469            0.843137          0.758303         0.084834  \n",
       "8            0.857143            0.823529          0.840336         0.016807  \n",
       "9            0.714286            0.901961          0.808123         0.093838  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show benchmark results.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    cv_results = pd.DataFrame(learner.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict testing data (only a sample of 100 to accelerate prediction process.)\n",
    "# the advantage of pipelining is that the raw testing data can be used and all pre-processing is done within it.\n",
    "learner.predict(X_test.head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Popcorn Bag)",
   "language": "python",
   "name": "popcorn-bag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
